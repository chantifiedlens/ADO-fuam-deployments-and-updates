{"cells":[{"cell_type":"markdown","source":["## Post-Deployment logic updated on July 18 2025\n","In this separate notebook, all the post-deployment tasks are performed, as well as some tasks absent from the fabric-cicd deployment."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f1e8b0d2-a877-41e8-928a-65855b12bd1f"},{"cell_type":"code","source":["%%configure -f \n","{   \"defaultLakehouse\": { \"name\": \"FUAM_Config_Lakehouse\" } }"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"91018a30-3b28-4ea3-80cb-093fb90a5c57","normalized_state":"finished","queued_time":"2025-07-17T17:27:29.0515967Z","session_start_time":"2025-07-17T17:27:29.2680455Z","execution_start_time":"2025-07-17T17:27:32.3500307Z","execution_finish_time":"2025-07-17T17:27:32.4229356Z","parent_msg_id":"f18d788f-ec15-4402-9f6a-b02c5fb310ef"}},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"60bb1703-82b2-4257-99c9-9b8d8b9e0ab6"},{"cell_type":"code","source":["%pip install ms-fabric-cli"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"91018a30-3b28-4ea3-80cb-093fb90a5c57","normalized_state":"finished","queued_time":"2025-07-17T17:27:29.3117633Z","session_start_time":null,"execution_start_time":"2025-07-17T17:27:33.2452961Z","execution_finish_time":"2025-07-17T17:27:42.0819551Z","parent_msg_id":"18d0d429-5ec7-4f43-b9b4-bad97c287036"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting ms-fabric-cli\n  Downloading ms_fabric_cli-1.0.1-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: msal<2,>=1.29 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal[broker]<2,>=1.29->ms-fabric-cli) (1.32.0)\nRequirement already satisfied: msal_extensions in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (1.2.0)\nCollecting questionary (from ms-fabric-cli)\n  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: prompt_toolkit>=3.0.41 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (3.0.50)\nRequirement already satisfied: cachetools>=5.5.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (5.5.2)\nRequirement already satisfied: jmespath in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (1.0.1)\nRequirement already satisfied: pyyaml==6.0.2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.32.3)\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.8.0)\nRequirement already satisfied: cryptography<47,>=2.5 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (44.0.2)\nRequirement already satisfied: wcwidth in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from prompt_toolkit>=3.0.41->ms-fabric-cli) (0.2.13)\nRequirement already satisfied: portalocker<3,>=1.4 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal_extensions->ms-fabric-cli) (2.10.1)\nRequirement already satisfied: cffi>=1.12 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from cryptography<47,>=2.5->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (1.17.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.2.2)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2024.7.4)\nRequirement already satisfied: pycparser in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from cffi>=1.12->cryptography<47,>=2.5->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.22)\nDownloading ms_fabric_cli-1.0.1-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.3/296.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading questionary-2.1.0-py3-none-any.whl (36 kB)\nInstalling collected packages: questionary, ms-fabric-cli\nSuccessfully installed ms-fabric-cli-1.0.1 questionary-2.1.0\nNote: you may need to restart the kernel to use updated packages.\n"]}],"execution_count":2,"metadata":{"jupyter":{"outputs_hidden":true},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"22e172d2-c00c-42d1-8c6a-3d993247a79e"},{"cell_type":"code","source":["pbi_connection_name = 'fuam pbi-service-api admin'\n","fabric_connection_name = 'fuam fabric-service-api admin'"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"a0bf6df1-7d8e-4105-8f41-d0ec58e17263"},{"cell_type":"markdown","source":["### Import of needed libaries"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"08bf3e85-fe99-4d19-9eea-48fc12211f48"},{"cell_type":"code","source":["import subprocess\n","import os\n","import json\n","from zipfile import ZipFile \n","import shutil\n","import re\n","import requests\n","import zipfile\n","from io import BytesIO\n","import yaml\n","import sempy.fabric as fabric\n","import time"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"91018a30-3b28-4ea3-80cb-093fb90a5c57","normalized_state":"finished","queued_time":"2025-07-17T17:27:29.5239399Z","session_start_time":null,"execution_start_time":"2025-07-17T17:27:42.0831808Z","execution_finish_time":"2025-07-17T17:27:49.5396063Z","parent_msg_id":"fa5b518b-a244-46f0-9812-e59717ae4d6f"}},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"0a03fb95-fe1f-4c6e-8bf1-a10099e76c70"},{"cell_type":"markdown","source":["## Download of source & config files\n","\n","This part downloads all source and config files of FUAM needed for the deployment into the ressources of the notebook"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"249a301a-ade2-4787-9f46-c7339ebcd156"},{"cell_type":"code","source":["def download_folder_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\"):\n","    # Construct the URL for the GitHub API to download the repository as a zip file\n","    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n","    \n","    # Make a request to the GitHub API\n","    response = requests.get(url)\n","    response.raise_for_status()\n","    \n","    # Ensure the directory for the output zip file exists\n","    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n","    \n","    # Create a zip file in memory\n","    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n","        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n","            for file_info in zipf.infolist():\n","                parts = file_info.filename.split('/')\n","                if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n","                    # Extract only the specified folder\n","                    file_data = zipf.read(file_info.filename)\n","                    output_zipf.writestr(('/'.join(parts[1:]).replace(remove_folder_prefix, \"\")), file_data)\n","\n","def uncompress_zip_to_folder(zip_path, extract_to):\n","    # Ensure the directory for extraction exists\n","    os.makedirs(extract_to, exist_ok=True)\n","    \n","    # Uncompress all files from the zip into the specified folder\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_to)\n","    \n","    # Delete the original zip file\n","    os.remove(zip_path)\n","\n","repo_owner = \"Microsoft\"\n","repo_name = \"fabric-toolbox\"\n","branch = \"main\"\n","folder_prefix = \"monitoring/fabric-unified-admin-monitoring\"\n","\n","download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/src/src.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/src\", remove_folder_prefix = f\"{folder_prefix}/\")\n","download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/config/config.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/config\" , remove_folder_prefix = folder_prefix)\n","download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/data/data.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/data\" , remove_folder_prefix = folder_prefix)\n","uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n","uncompress_zip_to_folder(zip_path = \"./builtin/data/data.zip\", extract_to= \"./builtin\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"91018a30-3b28-4ea3-80cb-093fb90a5c57","normalized_state":"finished","queued_time":"2025-07-17T17:27:29.6924569Z","session_start_time":null,"execution_start_time":"2025-07-17T17:27:49.5408217Z","execution_finish_time":"2025-07-17T17:28:31.4000354Z","parent_msg_id":"447d423d-2c8e-47c3-9132-78203e5e158a"}},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"de2b764b-c9e1-43c1-addd-d9daf09ec6f0"},{"cell_type":"code","source":["base_path = './builtin/'\n","config_path = os.path.join(base_path, 'config/deployment_config.yaml')\n","\n","with open(config_path, 'r') as file:\n","        config = yaml.safe_load(file)\n","\n","deploy_order_path = os.path.join(base_path, 'config/deployment_order.json')\n","with open(deploy_order_path, 'r') as file:\n","        deployment_order = json.load(file)\n","\n","src_workspace_name = config['workspace']\n","src_pbi_connection = config['connections']['pbi_connection']\n","src_fabric_connection = config['connections']['fabric_connection']\n","\n","semantic_model_connect_to_lakehouse = config['fuam_lakehouse_semantic_models']\n","\n","mapping_table=[]"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"343e6574-2c1e-4104-9e0a-7fca7809ad98"},{"cell_type":"markdown","source":["## Definition of deployment functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"e2392f0a-604d-4006-9793-0c74cf870e21"},{"cell_type":"code","source":["# Set environment parameters for Fabric CLI\n","token = notebookutils.credentials.getToken('pbi')\n","os.environ['FAB_TOKEN'] = token\n","os.environ['FAB_TOKEN_ONELAKE'] = token\n","\n","def run_fab_command(command, capture_output: bool = False, silently_continue: bool = False, timeout: int = 300):\n","    \"\"\"Run fabric CLI command with timeout protection (default 5 minutes).\"\"\"\n","    try:\n","        result = subprocess.run(\n","            [\"fab\", \"-c\", command], \n","            capture_output=capture_output, \n","            text=True, \n","            timeout=timeout\n","        )\n","        if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n","            raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result.stderr}'\")    \n","        if (capture_output): \n","            output = result.stdout.strip()\n","            return output\n","    except subprocess.TimeoutExpired:\n","        raise Exception(f\"Command timed out after {timeout} seconds: {command}\")\n","\n","def fab_get_id(name):\n","    id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/{name} -q id\" , capture_output = True, silently_continue= True)\n","    return(id)\n","\n","def get_id_by_name(name):\n","    for it in deployment_order:\n","        if it.get(\"name\") == name:\n","                return it.get(\"fuam_id\")\n","    return None\n","\n","\n","def copy_to_tmp(name):\n","    \"\"\"Extract files from zip to memory (handles nested folders at any depth).\"\"\"\n","    path2zip = \"./builtin/src/src.zip\"\n","    file_contents = {}  # Store file paths and their content in memory\n","    \n","    with ZipFile(path2zip) as archive:\n","        for file in archive.namelist():\n","            # Skip directory entries (ending with /) but include all files at any nesting level\n","            # This handles: src/name/file.txt, src/name/subfolder/file.txt, src/name/a/b/c/file.txt, etc.\n","            if file.startswith(f'src/{name}/') and not file.endswith('/'):\n","                # Read file content into memory instead of extracting to disk\n","                file_contents[file] = archive.read(file)\n","    \n","    return file_contents\n","\n","\n","def replace_ids_in_memory(file_contents, mapping_table):\n","    \"\"\"Replace IDs in memory-stored files.\"\"\"\n","    updated_contents = {}\n","    \n","    for file_path, content_bytes in file_contents.items():\n","        file_name = os.path.basename(file_path)\n","        \n","        # Decode bytes to string\n","        try:\n","            content = content_bytes.decode('utf-8')\n","        except:\n","            # If decoding fails, keep as binary\n","            updated_contents[file_path] = content_bytes\n","            continue\n","        \n","        if file_name.endswith('.ipynb'):\n","            notebook_json = json.loads(content)\n","            dependencies = notebook_json.get('metadata', {}).get('dependencies', {})\n","            depend = json.dumps(dependencies)\n","            for mapping in mapping_table:  \n","                depend = depend.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n","            notebook_json['metadata']['dependencies'] = json.loads(depend)\n","            content = json.dumps(notebook_json)\n","            \n","        elif file_name.endswith(('.py', '.json', '.pbir', '.platform', '.tmdl')) and not file_name.endswith('report.json'):\n","            for mapping in mapping_table:  \n","                content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n","        \n","        updated_contents[file_path] = content.encode('utf-8')\n","    \n","    return updated_contents\n","\n","def write_memory_to_temp(file_contents, temp_dir):\n","    \"\"\"Write in-memory files to temporary directory (system temp, not builtin storage).\"\"\"\n","    for file_path, content_bytes in file_contents.items():\n","        full_path = os.path.join(temp_dir, file_path)\n","        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n","        with open(full_path, 'wb') as f:\n","            f.write(content_bytes)\n","    return temp_dir\n","\n","def get_semantic_model_id_from_memory(file_contents, name):\n","    \"\"\"Get semantic model ID from in-memory report definition.\"\"\"\n","    definition_path = f'src/{name}/definition.pbir'\n","    if definition_path in file_contents:\n","        content = json.loads(file_contents[definition_path].decode('utf-8'))\n","        semantic_model_id = content.get('datasetReference', {}).get('byConnection', {}).get('pbiModelDatabaseName')\n","        if semantic_model_id:\n","            return semantic_model_id\n","    return None\n","\n","def get_semantic_model_id(report_folder):\n","    definition_file = os.path.join(report_folder, 'definition.pbir')\n","    if os.path.exists(definition_file):\n","        with open(definition_file, 'r', encoding='utf-8') as file:\n","            content = json.load(file)\n","            semantic_model_id = content.get('datasetReference', {}).get('byConnection', {}).get('pbiModelDatabaseName')\n","            if semantic_model_id:\n","                return semantic_model_id\n","    return None\n","\n","def update_sm_connection_to_fuam_lakehouse_in_memory(file_contents, name):\n","    \"\"\"Update semantic model connection to FUAM lakehouse in memory.\"\"\"\n","    new_sm_db = run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.connectionString\", capture_output=True, silently_continue=True)\n","    new_lakehouse_sql_id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.id\", capture_output=True, silently_continue=True)\n","    \n","    expressions_path = f'src/{name}/definition/expressions.tmdl'\n","    if expressions_path in file_contents:\n","        content = file_contents[expressions_path].decode('utf-8')\n","        match = re.search(r'Sql\\.Database\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', content)\n","        if match:\n","            old_sm_db, old_lakehouse_sql_id = match.group(1), match.group(2)\n","            content = content.replace(old_sm_db, new_sm_db).replace(old_lakehouse_sql_id, new_lakehouse_sql_id)\n","            file_contents[expressions_path] = content.encode('utf-8')\n","    return file_contents\n","\n","def update_sm_connection_to_fuam_lakehouse(semantic_model_folder):\n","    new_sm_db= run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.connectionString\", capture_output = True, silently_continue=True)\n","    new_lakehouse_sql_id= run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.id\", capture_output = True, silently_continue=True)\n","        \n","    expressions_file = os.path.join(semantic_model_folder, 'definition', 'expressions.tmdl')\n","    if os.path.exists(expressions_file):\n","        with open(expressions_file, 'r', encoding='utf-8') as file:\n","            content = file.read()\n","            match = re.search(r'Sql\\.Database\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', content)\n","            if match:\n","                old_sm_db, old_lakehouse_sql_id = match.group(1), match.group(2)\n","                content = content.replace(old_sm_db, new_sm_db).replace(old_lakehouse_sql_id, new_lakehouse_sql_id)\n","                with open(expressions_file, 'w', encoding='utf-8') as file:\n","                    file.write(content)\n","\n","\n","def update_report_definition_in_memory(file_contents, name):\n","    \"\"\"Update report definition in memory to reference semantic model by ID.\"\"\"\n","    semantic_model_id = get_semantic_model_id_from_memory(file_contents, name)\n","    definition_path = f\"src/{name}/definition.pbir\"\n","    \n","    if definition_path in file_contents and semantic_model_id:\n","        report_definition = json.loads(file_contents[definition_path].decode('utf-8'))\n","        \n","        # Update connection string to reference the semantic model by ID\n","        # Format: Data Source=powerbi://api.powerbi.com/v1.0/myorg/{workspace};initial catalog={model};semanticmodelid={id}\n","        connection_string = f\"Data Source=powerbi://api.powerbi.com/v1.0/myorg/{trg_workspace_name};initial catalog={{MODEL_NAME}};integrated security=ClaimsToken;semanticmodelid={semantic_model_id}\"\n","        \n","        # Ensure datasetReference structure exists\n","        if \"datasetReference\" not in report_definition:\n","            report_definition[\"datasetReference\"] = {}\n","        \n","        # Clear byPath if it exists\n","        if \"byPath\" in report_definition[\"datasetReference\"]:\n","            del report_definition[\"datasetReference\"][\"byPath\"]\n","        \n","        # Set byConnection with only the connectionString property\n","        report_definition[\"datasetReference\"][\"byConnection\"] = {\n","            \"connectionString\": connection_string\n","        }\n","        \n","        file_contents[definition_path] = json.dumps(report_definition, indent=4).encode('utf-8')\n","    \n","    return file_contents\n","\n","def update_report_definition(path): \n","    \"\"\"Update report definition to reference semantic model by ID (legacy disk-based version).\"\"\"\n","    semantic_model_id = get_semantic_model_id(path)\n","    definition_path = os.path.join(path, \"definition.pbir\")\n","   \n","    if semantic_model_id:\n","        with open(definition_path, \"r\", encoding=\"utf8\") as file:\n","            report_definition = json.load(file)\n","\n","        # Update connection string to reference the semantic model by ID\n","        connection_string = f\"Data Source=powerbi://api.powerbi.com/v1.0/myorg/{trg_workspace_name};initial catalog={{MODEL_NAME}};integrated security=ClaimsToken;semanticmodelid={semantic_model_id}\"\n","        \n","        # Ensure datasetReference structure exists\n","        if \"datasetReference\" not in report_definition:\n","            report_definition[\"datasetReference\"] = {}\n","        \n","        # Clear byPath if it exists\n","        if \"byPath\" in report_definition[\"datasetReference\"]:\n","            del report_definition[\"datasetReference\"][\"byPath\"]\n","        \n","        # Set byConnection with only the connectionString property\n","        report_definition[\"datasetReference\"][\"byConnection\"] = {\n","            \"connectionString\": connection_string\n","        }\n","\n","        with open(definition_path, \"w\") as file:\n","            json.dump(report_definition, file, indent=4)\n","\n","def print_color(text, state):\n","    red  = '\\033[91m'\n","    yellow = '\\033[93m'  \n","    green = '\\033[92m'   \n","    white = '\\033[0m'  \n","    if state == \"error\":\n","        print(red, text, white)\n","    elif state == \"warning\":\n","        print(yellow, text, white)\n","    elif state == \"success\":\n","        print(green, text, white)\n","    else:\n","        print(\"\", text)\n","\n"," "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"bffceeeb-88ef-444d-a177-df989daae803"},{"cell_type":"markdown","source":["## Creation of connections\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"35f47f8d-3a8d-40eb-ad47-1fdf8b3d6bdb"},{"cell_type":"code","source":["def create_or_get_connection(name, baseUrl, audience):\n","    try:\n","        run_fab_command(f\"\"\"create .connections/{name}.connection \n","            -P connectionDetails.type=WebForPipeline,connectionDetails.creationMethod=WebForPipeline.Contents,connectionDetails.parameters.baseUrl={baseUrl},connectionDetails.parameters.audience={audience},credentialDetails.type=Anonymous\"\"\")\n","        print_color(\"New connection created. Enter service principal credentials\", \"success\")\n","    except Exception as ex:\n","        print_color(\"Connection already exists\", \"warning\")\n","\n","    conn_id = run_fab_command(f\"get .connections/{name}.Connection -q id\", silently_continue= True, capture_output= True)\n","    print(\"Connection ID:\" + conn_id)\n","    \n","    \n","    return(conn_id)\n","    \n","conn_pbi_service_api_admin = create_or_get_connection(pbi_connection_name, \"https://api.powerbi.com/v1.0/myorg/admin\", \"https://analysis.windows.net/powerbi/api\" )\n","conn_fabric_service_api_admin = create_or_get_connection(fabric_connection_name, \"https://api.fabric.microsoft.com/v1/admin\", \"\thttps://api.fabric.microsoft.com\" )\n","\n","mapping_table.append({ \"old_id\": get_id_by_name(src_pbi_connection), \"new_id\": conn_pbi_service_api_admin })\n","mapping_table.append({ \"old_id\": get_id_by_name(src_fabric_connection), \"new_id\": conn_fabric_service_api_admin })"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"551e0598-5574-4ea1-bd20-2605d6666f93"},{"cell_type":"code","source":["# Set environment parameters for Fabric CLI\n","token = notebookutils.credentials.getToken('pbi')\n","os.environ['FAB_TOKEN'] = token\n","os.environ['FAB_TOKEN_ONELAKE'] = token\n","\n","def run_fab_command( command, capture_output: bool = False, silently_continue: bool = False):\n","    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n","    if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n","       raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result.stderr}'\")    \n","    if (capture_output): \n","        output = result.stdout.strip()\n","        return output"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"91018a30-3b28-4ea3-80cb-093fb90a5c57","normalized_state":"finished","queued_time":"2025-07-17T17:27:29.9002664Z","session_start_time":null,"execution_start_time":"2025-07-17T17:28:31.7137798Z","execution_finish_time":"2025-07-17T17:28:32.088308Z","parent_msg_id":"2729bf83-a274-4e62-a275-c1cf13dde319"}},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"49a3eaf4-dada-45f9-9dcb-7f68ac376d08"},{"cell_type":"code","source":["trg_workspace_id = fabric.get_notebook_workspace_id()\n","trg_workspace_id"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"91018a30-3b28-4ea3-80cb-093fb90a5c57","normalized_state":"finished","queued_time":"2025-07-17T17:27:30.0073287Z","session_start_time":null,"execution_start_time":"2025-07-17T17:28:32.0894649Z","execution_finish_time":"2025-07-17T17:28:32.4053628Z","parent_msg_id":"adf83015-6cbb-4177-86ed-3671a431ba6a"}},"metadata":{}},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"'6ed2dbfc-9882-46d2-80b3-edbc1e29851d'"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"d7873582-a4e7-4e67-83ff-750b0c7acdce"},{"cell_type":"code","source":["trg_workspace_id = fabric.get_notebook_workspace_id()\n","res = run_fab_command(f\"api -X get workspaces/{trg_workspace_id}\" , capture_output = True)\n","trg_workspace_name = json.loads(res)[\"text\"][\"displayName\"]\n","\n","print(f\"Current workspace: {trg_workspace_name}\")\n","print(f\"Current workspace ID: {trg_workspace_id}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"91018a30-3b28-4ea3-80cb-093fb90a5c57","normalized_state":"finished","queued_time":"2025-07-17T17:27:30.0973644Z","session_start_time":null,"execution_start_time":"2025-07-17T17:28:32.4065544Z","execution_finish_time":"2025-07-17T17:28:34.4738075Z","parent_msg_id":"2c821c80-ea47-4a7e-ba55-fdde97a92f69"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Current workspace: FUAM2Test\nCurrent workspace ID: 6ed2dbfc-9882-46d2-80b3-edbc1e29851d\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"0d12fe1d-d26b-411d-9b87-726536df4b9c"},{"cell_type":"code","source":["src_file_path = \"./builtin/data/table_definitions.snappy.parquet\"\n","with open(src_file_path, 'rb') as file:\n","                    content = file.read()\n","trg_lakehouse_folder_path = notebookutils.fs.getMountPath('/default') + \"/Files/table_definitions/\" \n","notebookutils.fs.mkdirs(f\"file://\" +trg_lakehouse_folder_path)\n","with open(trg_lakehouse_folder_path + \"table_definitions.snappy.parquet\", \"wb\") as f:\n","    f.write(content)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"91018a30-3b28-4ea3-80cb-093fb90a5c57","normalized_state":"finished","queued_time":"2025-07-17T17:27:30.1947518Z","session_start_time":null,"execution_start_time":"2025-07-17T17:28:34.4748042Z","execution_finish_time":"2025-07-17T17:28:35.7300309Z","parent_msg_id":"07e9dcc6-fc52-4697-8c4d-d67077ca9395"}},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"c3257437-01fc-4149-a9e6-210678eba47b"},{"cell_type":"code","source":["notebookutils.lakehouse.loadTable(\n","    {\n","        \"relativePath\": f\"Files/table_definitions/table_definitions.snappy.parquet\",\n","        \"pathType\": \"File\",\n","        \"mode\": \"Overwrite\",\n","        \"recursive\": False,\n","        \"formatOptions\": {\n","            \"format\": \"Parquet\"\n","        }\n","    }, \"FUAM_Table_Definitions\", \"FUAM_Config_Lakehouse\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"91018a30-3b28-4ea3-80cb-093fb90a5c57","normalized_state":"finished","queued_time":"2025-07-17T17:27:30.3013757Z","session_start_time":null,"execution_start_time":"2025-07-17T17:28:35.7312118Z","execution_finish_time":"2025-07-17T17:28:52.5839771Z","parent_msg_id":"589bd7cb-4a12-487e-92f2-74e85940fb04"}},"metadata":{}},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"09edbdcc-4c4f-418f-9714-48ff2018c880"},{"cell_type":"markdown","source":["In case the last step fails, please try to run it again or go to the Init_FUAM_Lakehouse_Tables Notebook and run it manually"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ad141615-240e-436f-a47e-599c09c1bd58"},{"cell_type":"code","source":["# Refresh SQL Endpoint for Config_Lakehouse\n","items = run_fab_command(f'api -X get -A fabric /workspaces/{trg_workspace_id}/items' , capture_output = True)\n","for it in json.loads(items)['text']['value']:\n","    if (it['displayName'] == 'FUAM_Config_Lakehouse' ) & (it['type'] =='SQLEndpoint' ):\n","        config_sql_endpoint = it['id']\n","    if (it['displayName'] == 'FUAM_Lakehouse' ) & (it['type'] =='SQLEndpoint' ):\n","        lh_sql_endpoint = it['id']\n","print(f\"FUAM_Lakehouse SQL Endpoint ID: {lh_sql_endpoint}\")\n","print(f\"FUAM_Config_Lakehouse SQL Endpoint ID: {config_sql_endpoint}\")\n","\n","try:\n","    run_fab_command(f'api -A fabric -X post workspaces/{trg_workspace_id}/sqlEndpoints/{config_sql_endpoint}/refreshMetadata?preview=True -i {{}} ', capture_output=True)\n","except:\n","    print(\"SQL Endpoint Refresh API failed, it is still in Preview, so there can be changes\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"5c31c2b2-e5b5-4270-be3a-f502dde554fd"},{"cell_type":"code","source":["# Fill default tables\n","time.sleep(10)\n","run_fab_command('job run ' + trg_workspace_name + '.Workspace/Init_FUAM_Lakehouse_Tables.Notebook -i {\"parameters\": {\"_inlineInstallationEnabled\": {\"type\": \"Bool\", \"value\": \"True\"} } }')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"editable":true,"run_control":{"frozen":false}},"id":"a41b7bf7-ba67-4cd2-b314-dce121b158ae"},{"cell_type":"code","source":["# Refresh of SQL Endpoint to make sure all tables are available\n","try:\n","    run_fab_command(f'api -A fabric -X post workspaces/{trg_workspace_id}/sqlEndpoints/{lh_sql_endpoint}/refreshMetadata?preview=True -i {{}} ', capture_output=True)\n","    print(\"Refresh FUAM_Lakehouse_SQL_Endpoint\")\n","except:\n","    print(\"SQL Endpoint Refresh API failed, it is still in Preview, so there can be changes\")\n","# Refresh Semantic Models on top of lakehouse\n","base_path = './builtin/'\n","config_path = os.path.join(base_path, 'config/deployment_config.yaml')\n","\n","with open(config_path, 'r') as file:\n","        config = yaml.safe_load(file)\n","\n","semantic_model_connect_to_lakehouse = config['fuam_lakehouse_semantic_models']\n","\n","for sm in semantic_model_connect_to_lakehouse:\n","    sm_id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/{sm} -q id\" , capture_output = True, silently_continue= True)\n","    run_fab_command(f'api -A powerbi -X post datasets/{sm_id}/refreshes -i  {{ \"retryCount\":\"3\" }} ')\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"editable":true,"run_control":{"frozen":false}},"id":"d1476d2b-783c-4a34-9711-ad522852334e"}],"metadata":{"kernel_info":{"jupyter_kernel_name":"python3.11","name":"jupyter"},"kernelspec":{"display_name":"Jupyter","language":"Jupyter","name":"jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}